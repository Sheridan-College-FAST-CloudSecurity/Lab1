{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7c3a77f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NumPy version: 2.4.0\n"
     ]
    }
   ],
   "source": [
    "# Setup\n",
    "import numpy as np\n",
    "np.set_printoptions(suppress=True, precision=3)\n",
    "\n",
    "def describe(name, x):\n",
    "    print(f\"{name}:\\n{x}\")\n",
    "    print(f\"shape: {x.shape}, dtype: {x.dtype}\\n\")\n",
    "\n",
    "print(\"NumPy version:\", np.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "501381f1",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 1: Broadcasting\n",
    "\n",
    "**Broadcasting** allows NumPy to perform operations on arrays of different shapes automatically.\n",
    "\n",
    "### Why Broadcasting Matters\n",
    "In neural networks, we often need to:\n",
    "- Add a **bias** (scalar or vector) to all samples.\n",
    "- Normalize data by subtracting mean or dividing by standard deviation.\n",
    "- Apply the same transformation across batches of data.\n",
    "\n",
    "Broadcasting makes these operations efficient and concise."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d25a6d3",
   "metadata": {},
   "source": [
    "### Example 1: Adding a Scalar to a Matrix\n",
    "When you add a scalar to a matrix, the scalar is \"broadcast\" to match the matrix shape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "595f7457",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X:\n",
      "[[ 1  2  3  4]\n",
      " [ 5  6  7  8]\n",
      " [ 9 10 11 12]]\n",
      "shape: (3, 4), dtype: int64\n",
      "\n",
      "X + 10 (broadcast scalar):\n",
      "[[11 12 13 14]\n",
      " [15 16 17 18]\n",
      " [19 20 21 22]]\n",
      "shape: (3, 4), dtype: int64\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Sample data: 3 samples, 4 features each\n",
    "X = np.array([\n",
    "    [1, 2, 3, 4],\n",
    "    [5, 6, 7, 8],\n",
    "    [9, 10, 11, 12]\n",
    "])\n",
    "describe(\"X\", X)\n",
    "\n",
    "# Add a scalar (like adding a bias term)\n",
    "X_biased = X + 10\n",
    "describe(\"X + 10 (broadcast scalar)\", X_biased)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31c82111",
   "metadata": {},
   "source": [
    "### Example 2: Adding a 1D Array to Each Row\n",
    "Broadcasting also works with 1D arrays. Each row of the matrix gets the same 1D array added to it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "75b36543",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bias:\n",
      "[0.1 0.2 0.3 0.4]\n",
      "shape: (4,), dtype: float64\n",
      "\n",
      "X + bias (broadcast 1D array):\n",
      "[[ 1.1  2.2  3.3  4.4]\n",
      " [ 5.1  6.2  7.3  8.4]\n",
      " [ 9.1 10.2 11.3 12.4]]\n",
      "shape: (3, 4), dtype: float64\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Bias vector: one bias per feature\n",
    "bias = np.array([0.1, 0.2, 0.3, 0.4])\n",
    "describe(\"bias\", bias)\n",
    "\n",
    "# Add bias to each row of X\n",
    "X_with_bias = X + bias\n",
    "describe(\"X + bias (broadcast 1D array)\", X_with_bias)\n",
    "\n",
    "# This is equivalent to adding the same bias to every sample!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5a90301",
   "metadata": {},
   "source": [
    "### Student Task: Broadcasting Practice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "775794b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Create a 3x3 matrix M\n",
    "M = np.array([    ])  # Add your 3 rows here\n",
    "\n",
    "# TODO: Multiply M by a scalar (e.g., 2.5)\n",
    "M_scaled = M *   # Complete this line\n",
    "describe(\"M scaled\", M_scaled)\n",
    "\n",
    "# TODO: Create a 1D array with 3 elements and add it to each row of M\n",
    "offset = np.array([    ])  # Add 3 numbers\n",
    "M_offset = M +   # Complete this\n",
    "describe(\"M + offset\", M_offset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "334ff88f",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 2: Matrix Multiplication for Data Transformation\n",
    "\n",
    "**Matrix multiplication** (`@` operator or `np.dot`) is the core operation in neural networks.\n",
    "\n",
    "### Why Matrix Multiplication?\n",
    "- Transforms input features into new representations.\n",
    "- Each output is a **weighted combination** of inputs.\n",
    "- Enables learning: the weights are adjusted during training.\n",
    "\n",
    "### Shape Rules\n",
    "For `A @ B`:\n",
    "- `A.shape = (m, n)`\n",
    "- `B.shape = (n, p)`\n",
    "- Result: `(m, p)`\n",
    "\n",
    "The inner dimensions must match!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "83a8d64e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_input:\n",
      "[[1 0 2]\n",
      " [0 1 1]]\n",
      "shape: (2, 3), dtype: int64\n",
      "\n",
      "W (weight matrix):\n",
      "[[0.5 0.3]\n",
      " [0.2 0.8]\n",
      " [0.1 0.4]]\n",
      "shape: (3, 2), dtype: float64\n",
      "\n",
      "X_input @ W:\n",
      "[[0.7 1.1]\n",
      " [0.3 1.2]]\n",
      "shape: (2, 2), dtype: float64\n",
      "\n",
      "Each output value is a weighted sum of input features!\n"
     ]
    }
   ],
   "source": [
    "# Input: 2 samples, 3 features\n",
    "X_input = np.array([\n",
    "    [1, 0, 2],  # sample 1\n",
    "    [0, 1, 1],  # sample 2\n",
    "])\n",
    "describe(\"X_input\", X_input)\n",
    "\n",
    "# Weight matrix: transforms 3 features â†’ 2 new features\n",
    "W = np.array([\n",
    "    [0.5, 0.3],  # weights for feature 0\n",
    "    [0.2, 0.8],  # weights for feature 1\n",
    "    [0.1, 0.4],  # weights for feature 2\n",
    "])\n",
    "describe(\"W (weight matrix)\", W)\n",
    "\n",
    "# Transform: X @ W â†’ (2, 3) @ (3, 2) = (2, 2)\n",
    "X_transformed = X_input @ W\n",
    "describe(\"X_input @ W\", X_transformed)\n",
    "\n",
    "print(\"Each output value is a weighted sum of input features!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ba54172",
   "metadata": {},
   "source": [
    "### Student Task: Matrix Multiplication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "531f3930",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Create a data matrix D with shape (4, 5) â€” 4 samples, 5 features\n",
    "D = np.array([    ])  # Add 4 rows of 5 numbers each\n",
    "\n",
    "# TODO: Create a weight matrix T with shape (5, 3) to transform 5 features â†’ 3\n",
    "T = np.array([    ])  # Add 5 rows of 3 numbers each\n",
    "\n",
    "# Perform transformation\n",
    "D_transformed = D @ T\n",
    "describe(\"D @ T\", D_transformed)\n",
    "print(f\"Expected shape: (4, 3), Actual shape: {D_transformed.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be969d82",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 3: Neural Network Intuition â€” What Are Weights?\n",
    "\n",
    "### Weights: The \"Knobs\" of Learning\n",
    "\n",
    "In a neural network:\n",
    "- **Weights** are numbers that control how much each input feature contributes to each output.\n",
    "- Initially, weights are random.\n",
    "- During training, weights are adjusted to minimize error (via gradient descent).\n",
    "- After training, weights encode the \"knowledge\" the network learned.\n",
    "\n",
    "### Analogy\n",
    "Think of weights as **volume knobs** on a mixing board:\n",
    "- Each input is a sound track.\n",
    "- Each weight controls how loud that track is in the final mix.\n",
    "- Training finds the right \"mix\" to produce the desired output.\n",
    "\n",
    "### Mathematical View\n",
    "For one output neuron:\n",
    "$$\n",
    "\\text{output} = w_1 \\cdot x_1 + w_2 \\cdot x_2 + \\ldots + w_n \\cdot x_n + b\n",
    "$$\n",
    "\n",
    "- $w_i$: weights (learnable)\n",
    "- $x_i$: input features\n",
    "- $b$: bias (learnable offset)\n",
    "\n",
    "In matrix form: `output = X @ W + b`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ea3544bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted price: $540,000\n",
      "\n",
      "Breakdown:\n",
      "  Size contribution: 300,000\n",
      "  Bedrooms contribution: 150,000\n",
      "  Age contribution: -10,000\n",
      "  Bias (base price): 100,000\n"
     ]
    }
   ],
   "source": [
    "# Example: predicting house price from [size, bedrooms, age]\n",
    "# Input: one house\n",
    "house = np.array([1500, 3, 10])  # sq ft, bedrooms, years old\n",
    "\n",
    "# Weights: learned importance of each feature\n",
    "weights = np.array([200, 50000, -1000])  # $ per sq ft, $ per bedroom, $ per year\n",
    "bias = 100000  # base price\n",
    "\n",
    "# Prediction (linear combination)\n",
    "price = np.dot(house, weights) + bias\n",
    "print(f\"Predicted price: ${price:,.0f}\")\n",
    "print(f\"\\nBreakdown:\")\n",
    "print(f\"  Size contribution: {house[0] * weights[0]:,.0f}\")\n",
    "print(f\"  Bedrooms contribution: {house[1] * weights[1]:,.0f}\")\n",
    "print(f\"  Age contribution: {house[2] * weights[2]:,.0f}\")\n",
    "print(f\"  Bias (base price): {bias:,.0f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae09ec97",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 4: Neural Network Intuition â€” What Are Activation Functions?\n",
    "\n",
    "### The Problem with Pure Linear Transformations\n",
    "\n",
    "If we only use `output = X @ W + b`, we have a **linear model**.\n",
    "- Linear models can only learn straight lines or flat planes.\n",
    "- Real-world patterns (images, language, etc.) are **non-linear**.\n",
    "\n",
    "**Key Insight**: Stacking multiple linear layers without non-linearity is equivalent to a single linear layer!\n",
    "\n",
    "### Activation Functions: Introducing Non-Linearity\n",
    "\n",
    "**Activation functions** apply element-wise non-linear transformations:\n",
    "$$\n",
    "\\text{output} = \\text{activation}(X @ W + b)\n",
    "$$\n",
    "\n",
    "This allows networks to learn complex, curved decision boundaries.\n",
    "\n",
    "### Common Activation Functions\n",
    "\n",
    "1. **ReLU (Rectified Linear Unit)**: `max(0, x)`\n",
    "   - Most popular in hidden layers.\n",
    "   - Keeps positive values, zeros out negatives.\n",
    "   - Fast and effective.\n",
    "\n",
    "2. **Sigmoid**: `1 / (1 + exp(-x))`\n",
    "   - Squashes values to (0, 1).\n",
    "   - Used for probabilities in binary classification.\n",
    "\n",
    "3. **Tanh**: `(exp(x) - exp(-x)) / (exp(x) + exp(-x))`\n",
    "   - Squashes values to (-1, 1).\n",
    "   - Centered around zero.\n",
    "\n",
    "4. **Softmax**: Converts logits to probability distribution.\n",
    "   - Used in the output layer for multi-class classification.\n",
    "\n",
    "### Role of Activation Functions\n",
    "- **Enable complexity**: Networks can approximate any function.\n",
    "- **Introduce non-linearity**: Break the chain of linear operations.\n",
    "- **Control output range**: Sigmoid/tanh for bounded outputs, ReLU for unbounded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3a45442a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input z: [-2 -1  0  1  2]\n",
      "ReLU(z): [0 0 0 1 2]\n",
      "Sigmoid(z): [0.119 0.269 0.5   0.731 0.881]\n",
      "Tanh(z): [-0.964 -0.762  0.     0.762  0.964]\n"
     ]
    }
   ],
   "source": [
    "# Define common activation functions\n",
    "def relu(x):\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def tanh(x):\n",
    "    return np.tanh(x)\n",
    "\n",
    "# Test on a range of values\n",
    "z = np.array([-2, -1, 0, 1, 2])\n",
    "print(\"Input z:\", z)\n",
    "print(\"ReLU(z):\", relu(z))\n",
    "print(\"Sigmoid(z):\", sigmoid(z))\n",
    "print(\"Tanh(z):\", tanh(z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb8e6d78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Create your own test vector with at least 5 values\n",
    "# Include negative, zero, and positive values\n",
    "test_vector = np.array([    ])  # Add your numbers here\n",
    "print(\"Test vector:\", test_vector)\n",
    "\n",
    "# TODO: Apply each activation function to your vector\n",
    "relu_result =   # Apply relu to test_vector\n",
    "sigmoid_result =   # Apply sigmoid to test_vector\n",
    "tanh_result =   # Apply tanh to test_vector\n",
    "\n",
    "print(\"\\nResults:\")\n",
    "print(\"ReLU:\", relu_result)\n",
    "print(\"Sigmoid:\", sigmoid_result)\n",
    "print(\"Tanh:\", tanh_result)\n",
    "\n",
    "# TODO: Test extreme values (very large positive/negative)\n",
    "extreme_values = np.array([-100, -10, 0, 10, 100])\n",
    "print(\"\\n--- Testing Extreme Values ---\")\n",
    "print(\"Input:\", extreme_values)\n",
    "print(\"ReLU (extreme):\", relu(extreme_values))\n",
    "print(\"Sigmoid (extreme):\", sigmoid(extreme_values))\n",
    "print(\"Tanh (extreme):\", tanh(extreme_values))\n",
    "\n",
    "# Observation questions:\n",
    "# 1. What happens to negative values in ReLU?\n",
    "# 2. What range do Sigmoid outputs fall in?\n",
    "# 3. What range do Tanh outputs fall in?\n",
    "# 4. Which activation \"saturates\" (approaches limits) for large inputs?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47e71be5",
   "metadata": {},
   "source": [
    "### Student Task: Experiment with Activation Functions\n",
    "Test how different activations transform various input ranges."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e0a7fc5",
   "metadata": {},
   "source": [
    "### Visualizing Activation Functions (Conceptual)\n",
    "If you were to plot these:\n",
    "- **ReLU**: A \"bent line\" â€” flat at 0 for x < 0, then slopes up.\n",
    "- **Sigmoid**: An \"S-curve\" from 0 to 1.\n",
    "- **Tanh**: An \"S-curve\" from -1 to 1.\n",
    "\n",
    "These non-linear shapes enable neural networks to learn curves, boundaries, and patterns."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4113b004",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 5: Simulating a Simple Neural Network Layer\n",
    "\n",
    "### A Neural Network Layer = Linear Transformation + Activation\n",
    "\n",
    "**Formula**:\n",
    "$$\n",
    "\\text{output} = \\text{activation}(X @ W + b)\n",
    "$$\n",
    "\n",
    "**Steps**:\n",
    "1. **Linear transformation**: Multiply input by weights (`X @ W`).\n",
    "2. **Add bias**: Shift the result (`+ b`).\n",
    "3. **Apply activation**: Introduce non-linearity (`activation(...)`).\n",
    "\n",
    "Let's simulate a small layer processing token embeddings!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91706aac",
   "metadata": {},
   "source": [
    "### Example: Processing Token Embeddings\n",
    "\n",
    "Imagine we have:\n",
    "- 3 tokens (words) in a sequence.\n",
    "- Each token is represented by a 4-dimensional embedding.\n",
    "- We want to transform embeddings to 5 dimensions using a neural layer.\n",
    "\n",
    "**Setup**:\n",
    "- Input: `(3, 4)` â€” 3 tokens, 4 features each.\n",
    "- Weights: `(4, 5)` â€” transform 4 â†’ 5.\n",
    "- Bias: `(5,)` â€” one bias per output feature.\n",
    "- Activation: ReLU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a949712b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input embeddings:\n",
      "[[ 0.5  0.2 -0.1  0.8]\n",
      " [ 0.3 -0.4  0.6  0.1]\n",
      " [-0.2  0.9  0.   0.5]]\n",
      "shape: (3, 4), dtype: float64\n",
      "\n",
      "Weight matrix W:\n",
      "[[ 0.499  0.66  -0.01  -0.683  0.251]\n",
      " [ 1.17   0.454  0.339  0.222 -1.098]\n",
      " [-0.707 -0.004  0.493 -0.025 -0.087]\n",
      " [-0.717  1.099  0.774 -0.249  0.31 ]]\n",
      "shape: (4, 5), dtype: float64\n",
      "\n",
      "Bias vector b:\n",
      "[-0.056  0.034  0.065  0.026  0.097]\n",
      "shape: (5,), dtype: float64\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Input embeddings: 3 tokens, 4-dimensional embeddings\n",
    "# (In real NLP, these come from an embedding layer)\n",
    "embeddings = np.array([\n",
    "    [0.5, 0.2, -0.1, 0.8],   # token 1: \"cloud\"\n",
    "    [0.3, -0.4, 0.6, 0.1],   # token 2: \"security\"\n",
    "    [-0.2, 0.9, 0.0, 0.5],   # token 3: \"data\"\n",
    "])\n",
    "describe(\"Input embeddings\", embeddings)\n",
    "\n",
    "# Weights: transform 4D â†’ 5D (learned during training)\n",
    "W_layer = np.random.randn(4, 5) * 0.5  # small random weights\n",
    "describe(\"Weight matrix W\", W_layer)\n",
    "\n",
    "# Bias: one per output dimension\n",
    "b_layer = np.random.randn(5) * 0.1\n",
    "describe(\"Bias vector b\", b_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a796d9b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear output z = X @ W + b:\n",
      "[[-0.076  1.334  0.698 -0.468  0.259]\n",
      " [-0.87   0.158  0.299 -0.307  0.59 ]\n",
      " [ 0.539  0.86   0.759  0.238 -0.787]]\n",
      "shape: (3, 5), dtype: float64\n",
      "\n",
      "Activated output = ReLU(z):\n",
      "[[0.    1.334 0.698 0.    0.259]\n",
      " [0.    0.158 0.299 0.    0.59 ]\n",
      " [0.539 0.86  0.759 0.238 0.   ]]\n",
      "shape: (3, 5), dtype: float64\n",
      "\n",
      "Notice: Negative values in z became 0 in output (ReLU effect)!\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Linear transformation\n",
    "z = embeddings @ W_layer + b_layer  # (3, 4) @ (4, 5) + (5,) â†’ (3, 5)\n",
    "describe(\"Linear output z = X @ W + b\", z)\n",
    "\n",
    "# Step 2: Apply activation (ReLU)\n",
    "output = relu(z)\n",
    "describe(\"Activated output = ReLU(z)\", output)\n",
    "\n",
    "print(\"Notice: Negative values in z became 0 in output (ReLU effect)!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3223c51e",
   "metadata": {},
   "source": [
    "### What Just Happened?\n",
    "\n",
    "1. **Weights mixed the input features**: Each output dimension is a weighted combination of the 4 input dimensions.\n",
    "2. **Bias shifted the result**: Added a learnable offset.\n",
    "3. **ReLU introduced non-linearity**: Negative values were zeroed out.\n",
    "\n",
    "This is **exactly** what happens in every layer of a neural network!\n",
    "\n",
    "### Deeper Networks\n",
    "In practice:\n",
    "- Stack **many** such layers.\n",
    "- Each layer learns increasingly abstract representations.\n",
    "- Early layers: edges, textures (for images) or simple patterns (for text).\n",
    "- Later layers: complex objects, concepts, semantic meaning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "144949da",
   "metadata": {},
   "source": [
    "### Student Task: Build Your Own Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b7de76c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Create input data with shape (5, 3) â€” 5 samples, 3 features\n",
    "X_student = np.array([    ])  # Add 5 rows of 3 numbers each\n",
    "\n",
    "# TODO: Create a weight matrix to transform 3 â†’ 4 features\n",
    "W_student = np.random.randn(  ,  ) * 0.5  # Fill in the shape\n",
    "\n",
    "# TODO: Create a bias vector with 4 elements\n",
    "b_student = np.random.randn(  ) * 0.1  # Fill in the size\n",
    "\n",
    "# Compute linear transformation\n",
    "z_student = X_student @ W_student + b_student\n",
    "describe(\"Linear output\", z_student)\n",
    "\n",
    "# TODO: Apply an activation function (choose: relu, sigmoid, or tanh)\n",
    "output_student =   # Apply activation to z_student\n",
    "describe(\"Activated output\", output_student)\n",
    "\n",
    "print(f\"Input shape: {X_student.shape} â†’ Output shape: {output_student.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54431a5f",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 6: Connecting to Real Neural Networks\n",
    "\n",
    "### Summary: What We've Learned\n",
    "\n",
    "1. **Broadcasting**: Efficiently applies operations across batches (adding bias, normalizing).\n",
    "2. **Matrix multiplication**: Core transformation in neural networks.\n",
    "3. **Weights**: Learnable parameters that encode knowledge.\n",
    "4. **Activation functions**: Non-linearities that enable complex pattern learning.\n",
    "5. **Neural layer**: `output = activation(X @ W + b)`.\n",
    "\n",
    "### Real-World Neural Networks\n",
    "\n",
    "In frameworks like PyTorch or TensorFlow:\n",
    "```python\n",
    "# PyTorch example (conceptual)\n",
    "layer = nn.Linear(in_features=4, out_features=5)  # W and b initialized\n",
    "output = torch.relu(layer(input))  # Apply linear + ReLU\n",
    "```\n",
    "\n",
    "Behind the scenes, it's doing **exactly** what we coded above!\n",
    "\n",
    "### Why This Matters \n",
    "\n",
    "- **Embeddings**: Words, code snippets, network traffic â†’ vectors.\n",
    "- **Transformers**: Stack attention + feed-forward layers (matrix ops + activations).\n",
    "- **Understanding internals**: Debug models, optimize performance, ensure reliability."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ecc25da",
   "metadata": {},
   "source": [
    "### Bonus Challenge: Two-Layer Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77baf444",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Build a 2-layer network\n",
    "# Input: (2, 3) â€” 2 samples, 3 features\n",
    "# Layer 1: 3 â†’ 5 features, ReLU activation\n",
    "# Layer 2: 5 â†’ 2 features, Sigmoid activation\n",
    "\n",
    "# Define input\n",
    "X_two_layer = np.array([\n",
    "    [1, 0, -1],\n",
    "    [0, 2, 1],\n",
    "])\n",
    "\n",
    "# Layer 1 weights and bias\n",
    "W1 = np.random.randn(3, 5) * 0.5\n",
    "b1 = np.random.randn(5) * 0.1\n",
    "\n",
    "# Layer 2 weights and bias\n",
    "W2 = np.random.randn(5, 2) * 0.5\n",
    "b2 = np.random.randn(2) * 0.1\n",
    "\n",
    "# Forward pass\n",
    "# TODO: Compute layer 1 output (use ReLU)\n",
    "h1 =   # hidden layer 1 output\n",
    "describe(\"Layer 1 output (hidden)\", h1)\n",
    "\n",
    "# TODO: Compute layer 2 output (use Sigmoid)\n",
    "h2 =   # final output\n",
    "describe(\"Layer 2 output (final)\", h2)\n",
    "\n",
    "print(\"This is a 2-layer neural network!\")\n",
    "print(\"Notice how the output is between 0 and 1 (sigmoid effect).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba787bf0",
   "metadata": {},
   "source": [
    "---\n",
    "## Summary & Key Takeaways\n",
    "\n",
    "### Broadcasting\n",
    "- Automatically expands smaller arrays to match larger ones.\n",
    "- Essential for adding biases and normalizing data.\n",
    "\n",
    "### Matrix Multiplication\n",
    "- Transforms data by computing weighted combinations.\n",
    "- Shape rule: `(m, n) @ (n, p) â†’ (m, p)`.\n",
    "\n",
    "### Weights\n",
    "- Learnable parameters that define transformations.\n",
    "- Encode the \"knowledge\" learned during training.\n",
    "- Adjusted via gradient descent to minimize loss.\n",
    "\n",
    "### Activation Functions\n",
    "- Introduce non-linearity, enabling complex patterns.\n",
    "- ReLU: most common in hidden layers.\n",
    "- Sigmoid/Softmax: used for probabilities in outputs.\n",
    "\n",
    "### Neural Network Layers\n",
    "- **Formula**: `output = activation(X @ W + b)`.\n",
    "- Each layer extracts higher-level features.\n",
    "- Stacking layers creates deep networks (deep learning).\n",
    "\n",
    "### Next Steps\n",
    "- Experiment with different weight initializations.\n",
    "- Try different activation functions and observe outputs.\n",
    "- Build deeper networks (3+ layers).\n",
    "- Explore real frameworks: PyTorch, TensorFlow, JAX.\n",
    "\n",
    "**You now understand the core mechanics of neural networks!** ðŸŽ‰"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
