{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6aa012dd",
   "metadata": {},
   "source": [
    "## Lab 4: From Matrices to Tensors\n",
    "Goal: build intuition for tensors (3D+ arrays), why they matter for AI/ML, and how shapes map to LLM workloads."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fde3f34b",
   "metadata": {},
   "source": [
    "### Terminology\n",
    "- Scalar (0D): single number. Think thermostat reading.\n",
    "- Vector (1D): ordered list. Think a checklist of counts.\n",
    "- Matrix (2D): rows x columns. Think spreadsheet table.\n",
    "- Tensor (3D+): stacked matrices. Think a shelf of spreadsheets where depth encodes batch or time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "028d60b9",
   "metadata": {},
   "source": [
    "### What are Embeddings?\n",
    "\n",
    "**Embeddings** are dense numeric vectors that represent tokens in a continuous space where similar meanings cluster together.\n",
    "\n",
    "**Analogy:** Think of a city map with coordinates. Each location (token) has GPS coordinates (embedding vector). Nearby places (similar words) have similar coordinates. \"King\" and \"Queen\" are close; \"King\" and \"Banana\" are far apart.\n",
    "\n",
    "**How it works:**\n",
    "1. Each token ID maps to a vector of numbers (typically 768, 1024, or 4096 dimensions).\n",
    "2. These vectors are learned during training to capture semantic relationships.\n",
    "3. Similar tokens have similar vectors (measured by cosine similarity or dot product).\n",
    "\n",
    "**Example:**\n",
    "```\n",
    "Token: \"security\" → Embedding: [0.23, -0.45, 0.87, ..., 0.12]  (768 dims)\n",
    "Token: \"safety\"   → Embedding: [0.21, -0.43, 0.89, ..., 0.15]  (close to security!)\n",
    "Token: \"cloud\"    → Embedding: [-0.67, 0.34, -0.21, ..., 0.45] (farther away)\n",
    "```\n",
    "\n",
    "**Why embeddings matter:**\n",
    "- They capture meaning: \"cat\" and \"kitten\" have similar embeddings.\n",
    "- They enable math on words: `king - man + woman ≈ queen`\n",
    "- They're the input to attention mechanisms and neural network layers.\n",
    "- Higher dimensions = more nuanced representations (but more memory/compute)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1bf83ff",
   "metadata": {},
   "source": [
    "### What are Tokens?\n",
    "\n",
    "**Tokens** are the basic units of text that AI models process. Think of tokenization as breaking sentences into digestible pieces—like cutting a sandwich into bite-sized chunks.\n",
    "\n",
    "**Analogy:** Imagine you're organizing a library. Instead of processing entire books at once, you catalog them by chapters, pages, or even individual words. Tokens work similarly: they break text into manageable units.\n",
    "\n",
    "**Examples:**\n",
    "- **Word-level tokens:** \"Hello world\" → `[\"Hello\", \"world\"]`\n",
    "- **Subword tokens:** \"unhappiness\" → `[\"un\", \"happiness\"]` or `[\"unhap\", \"piness\"]`\n",
    "- **Character tokens:** \"AI\" → `[\"A\", \"I\"]`\n",
    "\n",
    "Modern LLMs like GPT use subword tokenization (e.g., Byte-Pair Encoding). The sentence \"CloudSecurity rocks!\" might become `[\"Cloud\", \"Security\", \" rocks\", \"!\"]`. Each token gets a unique ID from the model's vocabulary (dictionary).\n",
    "\n",
    "**Why tokens matter:**\n",
    "- Models don't understand raw text—they need numeric representations.\n",
    "- Token limits (e.g., \"8K context window\") refer to the max number of tokens, not words.\n",
    "- Tokenization affects how models understand rare words, code, or multilingual text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5d16c36b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NumPy version: 2.4.0\n"
     ]
    }
   ],
   "source": [
    "# Setup\n",
    "import numpy as np\n",
    "np.set_printoptions(suppress=True, precision=3)\n",
    "\n",
    "def describe(name, x):\n",
    "    print(f\"{name}:\\n{x}\")\n",
    "    print(f\"shape: {x.shape}, dtype: {x.dtype}\\n\")\n",
    "\n",
    "print(\"NumPy version:\", np.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7d9c1f20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scalar:\n",
      "42\n",
      "shape: (), dtype: int64\n",
      "\n",
      "vector:\n",
      "[1 2 3]\n",
      "shape: (3,), dtype: int64\n",
      "\n",
      "matrix:\n",
      "[[1 2 3]\n",
      " [4 5 6]]\n",
      "shape: (2, 3), dtype: int64\n",
      "\n",
      "tensor3d:\n",
      "[[[ 0  1  2  3]\n",
      "  [ 4  5  6  7]\n",
      "  [ 8  9 10 11]]\n",
      "\n",
      " [[12 13 14 15]\n",
      "  [16 17 18 19]\n",
      "  [20 21 22 23]]]\n",
      "shape: (2, 3, 4), dtype: int64\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Scalars, vectors, matrices, tensors\n",
    "import numpy as np\n",
    "scalar = np.array(42)\n",
    "vector = np.array([1, 2, 3])\n",
    "matrix = np.array([[1, 2, 3], [4, 5, 6]])\n",
    "tensor3d = np.arange(2 * 3 * 4).reshape(2, 3, 4)  # shape: batches=2, rows=3, cols=4\n",
    "\n",
    "describe('scalar', scalar)\n",
    "describe('vector', vector)\n",
    "describe('matrix', matrix)\n",
    "describe('tensor3d', tensor3d)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "033dcdaa",
   "metadata": {},
   "source": [
    "### Tensors in LLMs\n",
    "- Each token is mapped to an embedding vector: shape `(embed_dim,)`.\n",
    "- A sequence of tokens becomes a 2D array: `(seq_len, embed_dim)`.\n",
    "- Multiple sequences in a batch become a 3D tensor: `(batch_size, seq_len, embed_dim)`.\n",
    "- Attention uses matrices (Q, K, V) derived from this tensor; understanding shapes prevents mismatches and explains limits like context windows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "89b051ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequence tokens: ['hello', 'ai', 'world']\n",
      "seq_embeddings:\n",
      "[[ 0.312 -0.721 -0.357  0.281]\n",
      " [-1.241  2.169 -1.149  0.253]\n",
      " [-1.299 -0.166 -1.589 -1.584]]\n",
      "shape: (3, 4), dtype: float64\n",
      "\n",
      "[[ 0.312 -0.721 -0.357  0.281]\n",
      " [-1.241  2.169 -1.149  0.253]\n",
      " [-1.299 -0.166 -1.589 -1.584]]\n"
     ]
    }
   ],
   "source": [
    "# Map a toy sequence to embeddings (2D)\n",
    "vocab = {'hello': 0, 'world': 1, 'ai': 2, 'cloud': 3}\n",
    "embed_dim = 4\n",
    "embedding_table = np.random.randn(len(vocab), embed_dim)\n",
    "\n",
    "sequence = ['hello', 'ai', 'world']\n",
    "token_ids = [vocab[t] for t in sequence]\n",
    "seq_embeddings = embedding_table[token_ids]  # shape (seq_len, embed_dim)\n",
    "\n",
    "print('Sequence tokens:', sequence)\n",
    "describe('seq_embeddings', seq_embeddings)\n",
    "print(seq_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6d34d91d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Padded sequences: [['hello', 'world', 'hello'], ['ai', 'cloud', 'world'], ['hello', 'ai', 'hello']]\n",
      "tensor_batch:\n",
      "[[[ 0.312 -0.721 -0.357  0.281]\n",
      "  [-1.299 -0.166 -1.589 -1.584]\n",
      "  [ 0.312 -0.721 -0.357  0.281]]\n",
      "\n",
      " [[-1.241  2.169 -1.149  0.253]\n",
      "  [ 1.459  0.803 -1.147  1.253]\n",
      "  [-1.299 -0.166 -1.589 -1.584]]\n",
      "\n",
      " [[ 0.312 -0.721 -0.357  0.281]\n",
      "  [-1.241  2.169 -1.149  0.253]\n",
      "  [ 0.312 -0.721 -0.357  0.281]]]\n",
      "shape: (3, 3, 4), dtype: float64\n",
      "\n",
      "tensor_batch sample for item 0: [[ 0.312 -0.721 -0.357  0.281]\n",
      " [-1.299 -0.166 -1.589 -1.584]\n",
      " [ 0.312 -0.721 -0.357  0.281]]\n"
     ]
    }
   ],
   "source": [
    "# Stack multiple sequences into a batch (3D)\n",
    "batch_sequences = [\n",
    "    ['hello', 'world'],\n",
    "    ['ai', 'cloud', 'world'],\n",
    "    ['hello', 'ai']\n",
    "]\n",
    "\n",
    "# Pad sequences to the same length for batching\n",
    "max_len = max(len(seq) for seq in batch_sequences)\n",
    "pad_token = 'hello'  # reuse an existing token for simplicity\n",
    "padded = [seq + [pad_token] * (max_len - len(seq)) for seq in batch_sequences]\n",
    "token_id_batch = [[vocab[t] for t in seq] for seq in padded]\n",
    "tensor_batch = embedding_table[token_id_batch]  # shape (batch_size, max_len, embed_dim)\n",
    "\n",
    "print('Padded sequences:', padded)\n",
    "describe('tensor_batch', tensor_batch)\n",
    "print('tensor_batch sample for item 0:', tensor_batch[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4675ee4",
   "metadata": {},
   "source": [
    "### Attention shape walkthrough\n",
    "From a 3D tensor `(batch, seq, embed)` we compute Q, K, V via matrix multiplies:\n",
    "- Q = X @ Wq, K = X @ Wk, V = X @ Wv where W* are `(embed, head_dim)`.\n",
    "- Attention scores use `Q @ K.T` per sequence; shapes must align.\n",
    "- Softmax over sequence length keeps each token attending within its context window."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e81c9eba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tiny attention demo on one sequence from the batch\n",
    "seq_example = tensor_batch[0]  # shape (seq_len, embed_dim)\n",
    "seq_len, embed_dim = seq_example.shape\n",
    "head_dim = 3\n",
    "\n",
    "Wq = np.random.randn(embed_dim, head_dim)\n",
    "Wk = np.random.randn(embed_dim, head_dim)\n",
    "Wv = np.random.randn(embed_dim, head_dim)\n",
    "\n",
    "Q = seq_example @ Wq  # (seq_len, head_dim)\n",
    "K = seq_example @ Wk  # (seq_len, head_dim)\n",
    "V = seq_example @ Wv  # (seq_len, head_dim)\n",
    "scores = Q @ K.T / np.sqrt(head_dim)  # (seq_len, seq_len)\n",
    "attention_weights = np.exp(scores) / np.exp(scores).sum(axis=-1, keepdims=True)\n",
    "context = attention_weights @ V  # (seq_len, head_dim)\n",
    "\n",
    "describe('Q', Q)\n",
    "describe('scores', scores)\n",
    "describe('attention_weights', attention_weights)\n",
    "describe('context', context)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "532c045d",
   "metadata": {},
   "source": [
    "### Why shapes matter\n",
    "- Context window: seq_len limits how far tokens can attend; truncation or padding changes tensor shapes.\n",
    "- Mixed data: stacking unrelated sequences in one batch can leak signals if masking is wrong.\n",
    "- Performance: larger batch or seq_len inflates tensor sizes, memory, and latency."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "155c0181",
   "metadata": {},
   "source": [
    "## Student TODO: Build your own tensor\n",
    "Create a batch of at least 2 sequences with 4-6 tokens each. Use a custom vocab and embedding dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2801d156",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: define your vocab and embeddings\n",
    "# vocab = {...}\n",
    "# embed_dim = ...\n",
    "# embedding_table = ...\n",
    "\n",
    "# TODO: define your sequences and batch them\n",
    "# sequences = [...]\n",
    "# pad and convert to ids\n",
    "# stacked = ...\n",
    "\n",
    "# TODO: describe shapes and maybe compute a simple aggregate (mean across tokens)\n",
    "# describe('my_tensor', stacked)\n",
    "# token_mean = stacked.mean(axis=1)  # average per sequence\n",
    "# print(token_mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef2f6a04",
   "metadata": {},
   "source": [
    "## Student TODO: Debug a shape mismatch\n",
    "Simulate an error by multiplying matrices with incompatible shapes, then fix it by reshaping or adjusting dims."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43bc9c16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: create two arrays with mismatched shapes, observe the error\n",
    "# a = np.random.randn(2, 3)\n",
    "# b = np.random.randn(4, 2)\n",
    "# try: a @ b\n",
    "# except Exception as e: print('error:', e)\n",
    "\n",
    "# TODO: fix the shapes (e.g., transpose b or change dimensions) and succeed\n",
    "# b_fixed = b.T  # adjust as needed\n",
    "# result = a @ b_fixed\n",
    "# describe('result', result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1523167a",
   "metadata": {},
   "source": [
    "---\n",
    "**Takeaways**\n",
    "- Tensors generalize vectors and matrices to handle batches, time, and features together.\n",
    "- LLMs rely on 3D tensors `(batch, seq, embed)` for embeddings and attention; shape fluency prevents bugs.\n",
    "- Context windows, padding, and masking are shape questions first, algorithm questions second."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
